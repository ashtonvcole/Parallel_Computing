\documentclass{article}
\usepackage{graphicx, float} % Required for inserting images
\usepackage[letterpaper, margin=1in]{geometry}
\usepackage{amsmath, listings, multirow, physics, placeins}

\lstdefinestyle{mystyle}{
	basicstyle=\ttfamily
}
\lstset{style=mystyle}

\title{A Linear Acoustic Perturbation Solver Using PETSc}
\author{Ashton Cole\\AVC687}
\date{\today}

\begin{document}

\maketitle

\section{Introduction}

This parallel computing final project implements a two-dimensional finite difference solver, which attempts to model the propagation of acoustic perturbations across a steady, approximately-incompressible flow. The solver is written in C, leveraging the PETSc linear algebra library.

The problem is inspired by my senior design project. In it, we were primarily tasked with physically testing an ultrasonic fluid flow rate sensor. The sensor operates with using transducers positioned at the sides of a pipe, with one upstream of the other. As they send signals back and forth, the waves move at the speed of sound relative to the flowing fluid medium. Because of this, the downstream signal travels faster than the upstream one, resulting in a measurable transit time difference, which can be used to infer the average fluid velocity.

In addition, we also wanted to model this effect computationally. First, we tried compressible flow simulations in OpenFOAM. However, when we tried adding viscosity, the solution became unstable. We reached out to Dr. Fabrizio Bisetti, a professor specializing in Computational Fluid Dynamics, and he suggested we try deriving our own linear system of partial differential equations to solve. This would be simpler, and hopefully easier, to solve than the Navier-Stokes equations.

Although we were not able to complete a Python-based finite element method solver during our project timeline, we were able to write a serial finite difference solver for the system of equations in one dimension, which proved that they work. I also decided to take on this same problem for this class' final project. Solving a system of partial differential equations is an ideal candidate for parallel programming, since the simulations require large matrix-vector computations. In addition, the PETSc library provides a convenient interface for parallel linear algebra operations in C. Instead of the finite element method, I again opted for a simpler finite difference method solver, but now on a two-dimensional structured grid. The program solves the equations in time explicitly and writes them to files within a case directory.

Unfortunately, the results were not particularly satisfying. In two dimensions, it seems that the solution very easily becomes unstable. However, speedup tests were at least able to show that the parallelization through PETSc offers some advantage for large systems.

\subsection{Setup and Execution}

In addition to a standard C compiler, this project requires an installation of PETSc, and by extension an implementation of MPI. Importantly, the \verb|$PETSC_DIR| and \verb|$PETSC_ARCH| environment variables should be set to the PETSc intallation location and build subdirectory, respectively.

To set up a particular case, constant values are defined in \verb|driver.c|, while functions are defined in \verb|driver_functions.c|. Once these are configured appropriately, the program may be built with CMake. This program may then either be executed in serial or parallel, using appropriate terminal commands. The case output directory will be written on a path relative to the present working directory.

\section{Methodology}

In this section, the derivation and discretization of the system of partial differential equations are discussed, as well as the implementation of the solver in C.

\subsection{Derivation of Partial Differential Equations}

After extended discussion with Dr. Bisetti, we decided to take the rather ambitious and exploratory route of deriving our own equations to solve. He presented the following idea, based on an acoustics textbook: start with the Euler equations, i.e. the inviscid, but compressible form of the Navier-Stokes equations (Equations \ref{eqn:com} and \ref{eqn:cop}). Assume that the velocity and density are the sum of a steady, compressible, viscous flow, and far smaller unsteady acoustic fluctuations. An isentropic pressure relation (Equation \ref{eqn:isen}) replaces the energy equation. This essentially means that we want to neglect nonlinear effects and dissapation over long distances, focusing on the transit time difference. Finally, the fluid is assumed to be an ideal gas. This is not representative of liquids like what our sensor would actually measure, but it was convenient to introduce the speed of sound into the system (Equation \ref{eqn:sos}). If anything, it would be just be a good place to start, and hopefully yield some meaningful results.

\begin{align}
	\pdv{\rho}{t} + \div{(\rho \vb{u})} &= 0 \label{eqn:com} \\
	\pdv{(\rho \vb{u})}{t} + \div{(\rho \vb{u} \otimes \vb{u})} &= - \grad{p} \label{eqn:cop} \\
	p &= C \rho^{\gamma} \label{eqn:isen} \\
	c &= \sqrt{\gamma \frac{p}{\rho}} \label{eqn:sos}
\end{align}

The following variable conventions are used for the above equations.

\begin{itemize}
	\item $\vb{x}$: position vector
	\item $t$: time
	\item $\rho(\vb{x}, t) = \overline{\rho} + \rho'(\vb{x}, t)$: density scalar field of the fluid, decomposed into a known constant $\overline{\rho}$ and a small acoustic perturbation $\rho'$, i.e. $\overline{\rho} \gg \rho'$
	\item $\vb{u}(\vb{x}, t) = \overline{\vb{u}}(\vb{x}) + \vb{u}'(\vb{x}, t)$: velocity vector field of the fluid, decomposed into a known steady $\overline{\vb{u}}$ and a small acoustic perturbation $\vb{u}'$, i.e. $\overline{\vb{u}} \gg \vb{u}'$
	\item $p(\vb{x}, t)$: pressure scalar field of the fluid, not decomposed for this derivation
	\item $\gamma$: ratio of specific heats of the fluid
	\item $c$: the ideal-gas speed of sound of the fluid
\end{itemize}

After substitutions, expansions, and neglecting small terms, the following linear, hyperbolic system of partial differential equations results (Equations \ref{eqn:rhop} and \ref{eqn:up}).

\begin{align}
	\pdv{\rho'}{t} + \overline{\rho} \div{\vb{u}'} + \overline{\vb{u}} \vdot \grad{\rho'} &= 0 \label{eqn:rhop} \\
	\pdv{\vb{u}'}{t} + \overline{\vb{u}} \vdot \grad{\vb{u}'} + \frac{c^{2}}{\overline{\rho}} \grad{\rho'} &= - \overline{\vb{u}} \vdot \grad{\overline{\vb{u}}} \label{eqn:up}
\end{align}

\subsection{Problem Definition}

The problem is to solve the above system of equations in two dimensions over a rectangular domain $\Omega = [x_{a}, x_{b}] \cross [y_{a}, y_{b}]$ from time $t_{a}$ to time $t_{b}$. The solution has an initial state $\rho'(\vb{x}, t_{a}) = \rho'_{0}(\vb{x})$ and $\vb{u}'(\vb{x}, t_{a}) = \vb{u}'_{0}(\vb{x})$. Each of the four boundaries has Dirichlet conditions imposed on $\rho'$ and $\vb{u}'$, which are permitted to be functions of both space and time. Finally, the incompressible flow field solutions $\overline{\rho}$ and $\overline{\vb{u}}$ are prescribed.

\subsection{Discrete Formulation}

The solution is solved discretely at rectilinear, regularly-spaced grid points $(x_{i}, y_{j})$. The solution variable values at these points are combined into a single vector $\vb{z} = (\rho'_{0, 0}, u'_{0, 0}, v'_{0, 0}, \rho'_{0, 1}, u'_{0, 1}, v'_{0, 1}, \dots)$, with an indexing method $e = 3 n_{y} i + 3j + k$, where $k$ is the variable index and $n_{y}$ is the number of solution points along the $y$-direction. This discretization allows the system of partial differential equations, deploying second-order-accurate central finite differences, to be approximately rewritten as a system of ordinary differential equations (Equation \ref{eqn:ode}). The exception is at the boundary nodes, but these equations will be adjusted later.

\begin{align}
	\dv{\vb{z}}{t} &= \vb{A} \vb{z} + \vb{b} \label{eqn:ode}
\end{align}

Then, central finite differences are again used to approximate the time derivative. This allows for an explicit definition for $\vb{z}$ at the next time step $n$ (Equation \ref{eqn:leap}).

\begin{align}
	\frac{\vb{z}^{n} - \vb{z}^{n - 2}}{\Delta t} &= \vb{A} \vb{z}^{n - 1} + \vb{b} \\
	\vb{z}^{n} &= \vb{z}^{n - 2} + \Delta t (\vb{A} \vb{z}^{n - 1} + \vb{b}) \label{eqn:leap}
\end{align}

Because this method requires two solution steps to calculate, the first new step is calculated with the forward Euler method.

\subsection{Implementation with PETSc}

The Portable, Extensible Toolkit for Scientific Computation (PETSc) library is used to conduct all necessary matrix-vector operations in C. As an added benefit, it supports computations in parallel, being built on top of the Message-Passing Interface (MPI) standard. Thus, this solver supports vectors, matrices, and calculations spread out across multiple concurrently-operating processors, expediting the solution of large problems.

The whole program uses the SPMD model characteristic of MPI parallelization, with each processor executes the same instructions. Some commands, like printing to the terminal or looping for vector and matrix value assignment, are only executed in serial on the lowest-rank processor.

It is worth noting that the inter-processor communication routines for vector-matrix operations are hidden under the PETSc interface. For example, consider the following snippet of code from \verb|_build_matrix()| in \verb|acoustic_problem.c| showing specifically the sequence of commands used to assemble the $\vb{A}$ matrix.

\begin{lstlisting}[language=C]
Mat A;
PetscCall(MatCreate(comm, &A));
PetscCall(MatSetType(A, MATAIJ)); // Or MATMPIAIJ
PetscInt matrix_size = ac.sd.nx * ac.sd.ny * 3;
PetscCall(MatSetSizes(A, PETSC_DECIDE, PETSC_DECIDE, matrix_size, matrix_size));

// Loop through domain, bc, and set values by adding
if (procno == 0) {
	// Call MatSetValues()
}

PetscCall(MatAssemblyBegin(A, MAT_FINAL_ASSEMBLY));
PetscCall(MatAssemblyEnd(A, MAT_FINAL_ASSEMBLY));
\end{lstlisting}

Essentially, sections of the matrix are allocated on each process, but all of the values are set on the 0-rank process. These have to be distributed appropriately using message passing. The assembly begin and end functions clearly mirror how, in MPI, sending and receiving have to be coordinated with \verb|MPI_isend()| and \verb|MPI_irecv()| across processes to prevent blocking. However, none of these MPI commands have to be actually used. They are already implemented within the PETSc abstract interface.

\subsection{Output}

The main program outputs the results to a case directory. Within this directory, simulation parameters are written to a JSON metatdata file, while the time and solution vector are written to time step subdirectories. Visualizations are generated using a Python postprocessing script.

\section{Results}

Both visual inspection of the results and parallel speedup tests are used to evaluate the program.

\subsection{Visualizations}

\FloatBarrier

First, a two-dimensional version of a wave pulse considered in one dimension is tested (Figure \ref{fig:replicate_1d_rho_p}). The underlying flow is $\overline{\rho} = 1$ and $\overline{\vb{u}} = (0.1, 0)$. Unfortunately, unlike in the one-dimensional case with the same parameters, oscillations quickly develop.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.5\linewidth]{images/replicate_1d_rho_p_0.png}
	\includegraphics[width=0.5\linewidth]{images/replicate_1d_rho_p_10.png}
	\includegraphics[width=0.5\linewidth]{images/replicate_1d_rho_p_20.png}
	\caption{Wave Pulse: Plots for $\rho'$ at times $t = 0, 0.1, 0.2$ s.}
	\label{fig:replicate_1d_rho_p}
\end{figure}

Then, a case is attempted with a circle of positive perturbation at the center of the domain (Figure \ref{fig:pulse_rho_p}). Here too, instabilities quickly take over.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.5\linewidth]{images/pulse_rho_p_0.png}
	\includegraphics[width=0.5\linewidth]{images/pulse_rho_p_10.png}
	\includegraphics[width=0.5\linewidth]{images/pulse_rho_p_20.png}
	\caption{Circular Pulse: Plots for $\rho'$ at times $t = 0, 0.1, 0.2$ s.}
	\label{fig:pulse_rho_p}
\end{figure}

\FloatBarrier

\subsection{Performance tests}

The second case was re-executed at different levels of refinement, with different numbers of processors, with and without file output, for parallel speedup tests.

\begin{table}[h!]
	\centering
	\caption{Performance tests for a simple case.}
	\label{tbl:perftests}
	\begin{tabular}{r|ccccc}
		Processes & \multicolumn{5}{c}{Number of Lateral Grid Points $(n_{x}, n_{y})$} \\
		 & 101 & 202 & 303 & 404 & 505 \\
		\hline
		 & \multicolumn{4}{c}{With file writing enabled} \\
		\hline
		1 & 13.239 s & 52.324 s & 116.074 s &  &  \\
		2 & 13.357 s & 51.957 s & 116.437 s &  &  \\
		4 & 13.260 s & 53.227 s & 114.786 s &  &  \\
		8 & 14.563 s & 56.279 s & 128.568 s &  &  \\
		\hline
		 & \multicolumn{4}{c}{With file writing disabled} \\
		\hline
		1 & 1.310 s & 3.781 s & 7.705 s & 13.343 s & 20.549 s \\
		2 & 1.167 s & 3.593 s & 7.557 s & 13.038 s & 19.834 s \\
		4 & 1.034 s & 2.955 s & 6.118 s & 10.583 s & 16.110 s \\
		8 & 1.123 s & 3.138 s & 6.347 s & 10.791 s & 16.176 s
	\end{tabular}
\end{table}

\section{Conclusions}

Unfortunately, this two-dimensional solver did not provide the satisfying results that were hoped for. The matrix assembly code and matrix itself were inspected carefully to ensure that they were built as expected. The oscillations suggest that the issue is one of numerical stability. It is possible that additional refinements with even smaller time steps will yield stable solutions, or the system may need to be solved with a different, perhaps implicit, method.

However, at least some insights may be gleaned from the parallelization tests. At around $n_{x} = n_{y} = 303$, speedup advantage from parallel operations starts to appear. However, the biggest impact is from disabling file writing. This is because, when writing to file, the vector must be ordered correctly. Each process must write the vector in the correct sequence, bringing the execution time back to the level of a sequential program.

Nevertheless, even without file writing, the speedup is far from inversely proportional to the number of processors. The advantage actually seems to level off between 4 and 8 processors. This could be due to the sequential loops which create $\vb{A}$ and $\vb{b}$, but another possibility is messgage passing during the matrix-vector operations. Because the grid is two-dimensional, the stencil of finite differences for a given entry of $\vb{z}$ include entries that, even though they are off-index by only one unit of $i$, are around $3 n_{y}$ spaces away from each other. This makes them likely to be stored on separate processes, entailing message passing latency for each calculation of $\vb{A} \vb{z}$.

\end{document}